{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea674d59",
   "metadata": {},
   "source": [
    "# Intro to Natural Language Processing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f79a17c",
   "metadata": {},
   "source": [
    "> \"You shall know a word by the company it keeps.\" ~ John R. Firth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "3bef7a43-b8fa-46fa-ad0f-846b1b95e4b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<iframe width=\"768\" height=\"432\" src=\"https://miro.com/app/live-embed/uXjVOlC3sTw=/?moveToViewport=-1354,-1121,2108,1681&embedId=334819522676\" frameborder=\"0\" scrolling=\"no\" allowfullscreen></iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "\n",
    "<iframe width=\"768\" height=\"432\" src=\"https://miro.com/app/live-embed/uXjVOlC3sTw=/?moveToViewport=-1354,-1121,2108,1681&embedId=334819522676\" frameborder=\"0\" scrolling=\"no\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4086650d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57d76b4",
   "metadata": {
    "tags": []
   },
   "source": [
    "The goal of this short demo is to cover the process of preparing and transforming text data in order to build a similarity based recommender system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3812d3aa",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e068da",
   "metadata": {},
   "source": [
    "1. Libraries\n",
    "2. The Data\n",
    "3. Flash NLP Intro\n",
    "4. Cleaning\n",
    "5. Recommendation System\n",
    "6. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77dd8621",
   "metadata": {},
   "source": [
    "## 1. Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34491fee",
   "metadata": {},
   "source": [
    "Download the following libraries, if not available already. You can check with `!pip list` or with `!conda list` in a new cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0e7ea5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install -U spacy panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970a5d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, re, spacy\n",
    "from random import choice\n",
    "import pandas as pd, numpy as np\n",
    "from pprint import pprint\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import panel as pn\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "pn.extension()\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc67fb6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46249716",
   "metadata": {},
   "source": [
    "With have been given a random corpus of news articles, plus some additional information (outlined below), and we want to make a useful product with it.\n",
    "\n",
    "| Column | Content |\n",
    "|--------|---------|\n",
    "|title |Title of article|\n",
    "|text | Text inside article|\n",
    "|domain | Domain Url of article|\n",
    "|date | YYYY-MM-DD Time|\n",
    "|description | Abstract of article|\n",
    "|url | Url of article|\n",
    "|image_url | Image if available|\n",
    "\n",
    "Here is the full description of the dataset from HugginFace.\n",
    "\n",
    "> \"CC-News dataset contains news articles from news sites all over the world. The data is available on AWS S3 in the Common Crawl bucket at /crawl-data/CC-NEWS/. This version of the dataset has been prepared using news-please - an integrated web crawler and information extractor for news.\n",
    "It contains 708241 English language news articles published between Jan 2017 and December 2019. It represents a small portion of the English language subset of the CC-News dataset.\" ~ [Hugging Face cc_news](https://huggingface.co/datasets/cc_news)\n",
    "\n",
    "Before we do any data cleaning, let's read in the data and explore it a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133caa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"cc_news_sample.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f4844a",
   "metadata": {},
   "source": [
    "Let's see how many articles we have and then examine the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e9547f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ceaea9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eaacefe",
   "metadata": {},
   "source": [
    "## 3. Flash NLP Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ace9fa",
   "metadata": {},
   "source": [
    "Let's pick a random article using `.loc[index, column]` on our dataframe and let's examine it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539bb68f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "random_article = df.iloc[choice(range(5000)), 1]\n",
    "pprint(random_article)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e408ec",
   "metadata": {},
   "source": [
    "Notice how the review above looks a bit odd and it has a few characters that will not be useful for our analysis. Let's examine a cleaner version of the article above by running it through `spaCy`'s tokenizer.\n",
    "\n",
    "When we tokenize a document, we are separating all of its content into each of its components, i.e. words, numbers, punctiations and the like, to make it easier to process it, clean it, transform it and to run computations on it.\n",
    "\n",
    "For this part, we will load an english model, instantiate it and pass an example article through it. You may need to run the cell below first to download the english model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c55811",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78c1b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4e249a",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_article = nlp(random_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e106529c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "parsed_article"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a65af8f",
   "metadata": {},
   "source": [
    "Notice how much nicer our article looks now.\n",
    "\n",
    "We can also grab the sentences and view them one by one using the attribute `.sents` and the built in python function `next()`, since the attribute of a document that has been tokenized by spacy will always return an iterator. Conversely, we can add it to a loop and show each of the sentences in an article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbfb882",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(enumerate(parsed_article.sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7d2e28",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for num, sentence in enumerate(parsed_article.sents):\n",
    "    print(f\"Sentence #{num}:\\n {sentence}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc73198",
   "metadata": {},
   "source": [
    "We can also have a look at the different kinds of entities in an article. These entities can be a person (called PERSON), and number (called CARDINAL), a geopolitical entity (called GPE), etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3856d5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for num, entity in enumerate(parsed_article.ents):\n",
    "    print(f\"Entity #{num}: {entity} -- {entity.label_}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bd5c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.explain(\"LOC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac41a04",
   "metadata": {},
   "source": [
    "We can also check weather a word is a stopword or a punctuation, or we can even lemmatize our articles. Lemmatization is a way of taking the root of a word and bringing similar words to a common denominator, for example, `was` will become `be` and most plural words will become singular words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5cf574",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_list = []\n",
    "\n",
    "for token in parsed_article:\n",
    "    new_list.append(token.text)\n",
    "    \n",
    "    \n",
    "new_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2060f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_list = [token.text for token in parsed_article]\n",
    "\n",
    "new_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47510924",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# here we are taking out of the parsed article each token\n",
    "token_text = [token.text for token in parsed_article]\n",
    "\n",
    "# here we are lemmatizing each word possible\n",
    "token_lemmas = [token.lemma_ for token in parsed_article]\n",
    "\n",
    "# stopwords are very common so here we will extract a variable that will tell us whether\n",
    "# a token is a stopword or not\n",
    "token_stop = [token.is_stop for token in parsed_article]\n",
    "\n",
    "# a token is a pinctuation or not\n",
    "token_punc = [token.is_punct for token in parsed_article]\n",
    "\n",
    "# we will now add all three to a dataframe and display it without assigning it to a variable\n",
    "pd.DataFrame(zip(token_text, token_lemmas, token_punc, token_stop), columns=['Original Text', 'Lemmatized Text', 'Punctuations', 'stopwords']).head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acded81",
   "metadata": {},
   "source": [
    "## 4. Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9451715d",
   "metadata": {},
   "source": [
    "Let's start by checking if our dataset contains any missin values, and then evaluate the amount of memory we are currently using from our machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b198ddb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df3460d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d00ab65",
   "metadata": {},
   "source": [
    "Depending on the random sample you choose at the beginning, you may or may not have a lot. If so, getting rid of the columns you don't need will help release some of the memory in your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfcee06",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['url', 'image_url', 'domain'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e183d4f1",
   "metadata": {},
   "source": [
    "Perfect! Let's now extract the `text` column and normalize it. This means we will use `spacy` to,\n",
    "- take out anything that is not a word or a number,\n",
    "- convert to lower case,\n",
    "- strip the spaces around the words,\n",
    "- tokenize the articles,\n",
    "- remove stopwords (we will use spaCy's list of stopwords for this),\n",
    "- and then join the cleaned tokens back together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0547d647",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = df['text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148583c0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "len(STOP_WORDS), STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3cc623",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_doc(doc):\n",
    "    \"\"\"\n",
    "    This function normalizes your list of documents by taking only\n",
    "    words, numbers, and spaces in between them. It then filters out\n",
    "    stop words.\n",
    "    \"\"\"\n",
    "    doc = re.sub(r'[^a-zA-Z0-9\\s]', '', doc, re.I|re.A)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    tokens = nlp(doc)\n",
    "    filtered_tokens = [token.lemma_ for token in tokens if not token.is_stop]\n",
    "    doc = ' '.join(filtered_tokens).replace(\" \\n \", \"\")\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f144da",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "random_article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d60c85",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "normalize_doc(random_article)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a3f444",
   "metadata": {},
   "source": [
    "Since we have quite a few articles, this operation can take quite some time unless we do the cleaning process concurrently or in parallel. We will do this using the `ProcessPoolExecutor()` from the `concurrent.futures` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844c3eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# with ProcessPoolExecutor() as e:\n",
    "#     processed_articles = list(e.map(normalize_doc, articles))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501d7246",
   "metadata": {},
   "source": [
    "We will add the cleaned versions of the documents back into the dataframe and loop over these while taking the lenght (in characters terms) of each article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607a9a96-be6e-4117-8d84-c014324ebb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_articles = pd.read_csv(\"processed_articles.csv.gz\")\n",
    "processed_articles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd33ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_text'] = processed_articles.values\n",
    "df['len_clean_text'] = df['clean_text'].apply(len)\n",
    "df['len_dirty_text'] = df['text'].apply(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2430f0",
   "metadata": {},
   "source": [
    "Let's now save our cleaned dataset in case we need to restart our notebook and begin the analysis again. We will also release a bit of memory by getting rid of all the data and variables we have loaded up since the beginning of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a90d73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476466c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['title', 'date', 'clean_text', 'len_clean_text', 'len_dirty_text']].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75a0328",
   "metadata": {},
   "source": [
    "It wouldn't make any sense to feed to our algorithms articles with a tiny amount of characters, so let's examine the distribution of characters among both, the raw and the clean version of our articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113abbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['len_clean_text', 'len_dirty_text']].describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7b5c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['len_clean_text', 'len_dirty_text']].skew()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa97a0c",
   "metadata": {},
   "source": [
    "![img](https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fcdn.analyticsvidhya.com%2Fwp-content%2Fuploads%2F2020%2F06%2Fsk1.png&f=1&nofb=1)\n",
    "\n",
    "Now that we know we have a skewed distribution of characters, let's fix that by setting up a rule. We'll evaluate an article using the tweets' maximum character count of 280, at the time of writing, and filter out all articles with less than that. Let's check how many we have first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f02e8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "greater_than_a_tweet = df['len_clean_text'] > 280\n",
    "print(f\"Before: {df.shape[0]} --- After: {greater_than_a_tweet.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18bb1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[greater_than_a_tweet].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb689198",
   "metadata": {},
   "source": [
    "# 5. Recommendation System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5604cfd5",
   "metadata": {},
   "source": [
    "Recommendation systems can come in many different forms and sizes. We can create a system that takes into account the behaviour of other users, or a system that only looks at similar articles or items to make a recommendation. Both are powerful systems and could cover an entire section of a book in their own right, which is why we will focus on the latter category, the one that makes recommendations based on similar articles.\n",
    "\n",
    "To create our recommendation system we first need to convert our articles into a numerical representation. We do this with a so-called bag of words (bow). BOWs are matrices with the documents in the rows, the terms contained in all documents along the columns. The frequency with which each term appears in each document along the values can be found in the doc-token combination. To create this kind of representation we can use `sklearn`'s `CountVectorizer` or `TfidfVectorizer` classes. The latter being the normalized version of the former, i.e. the frequency of a word divided by the amount of documents in which it appears.\n",
    "\n",
    "To use this classes we first instantiate them, fit the data to them so that they can learn the vocabulary of our corpus, and then we tranform the corpus into a sparse matrix. These sparse matrices hold the location of all non-zero values to make it easier to store the data and compute on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8db81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# we first instantiate our class\n",
    "tf = TfidfVectorizer(min_df=0.035, max_df=0.80)\n",
    "\n",
    "# we can fit and transform the data in the same step\n",
    "tfidf_matrix = tf.fit_transform(df['clean_text'].values)\n",
    "\n",
    "# evaluate the shape of our matrix\n",
    "tfidf_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368bfa3b",
   "metadata": {},
   "source": [
    "We can access our vocabulary with `.get_feature_names()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaccc68b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf.get_feature_names()[500:550]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e8833b",
   "metadata": {},
   "source": [
    "The next step is to get the distance between documents and words to see how close and how far, based on words only, are two documents from one another. The `cosine_similarity` function we imported earlier can do this for us, and afterwards, we can create a dataframe to evaluate our results.\n",
    "\n",
    "**Note:** this operation can take a few minutes if you are using the entire dataset. Make sure to grab some ‚òïÔ∏è üòé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d148cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "doc_sim = cosine_similarity(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e0e4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_sim_df = pd.DataFrame(doc_sim)\n",
    "doc_sim_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f7dcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_sim.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d4feee",
   "metadata": {},
   "source": [
    "The reason we see a X000xX000 matrix is because both halfs alonside the diagonal line are identical, hence, we have the similarity of all docs vs all docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5568bf",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "articles_list = df['title'].values\n",
    "articles_list.shape, articles_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5320581",
   "metadata": {},
   "source": [
    "Let's now\n",
    "1. pick a title at random\n",
    "2. get the index of such title\n",
    "3. select the corresponding row for such title in our new document similarity dataframe\n",
    "4. sort the index of such values\n",
    "5. return the top 5 article titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebcb555",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_title = choice(articles_list)\n",
    "a_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2ad868",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_idx = np.where(articles_list == a_title)[0][0]\n",
    "article_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4352fa4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "article_similarities = doc_sim_df.iloc[article_idx].values\n",
    "article_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2f7e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that we don't select the first one as this should always be one\n",
    "similar_articles_idxs = np.argsort(-article_similarities)[1:10]\n",
    "similar_articles_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ee2055",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73d7d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = nlp(df.iloc[1, 2])\n",
    "doc2 = nlp(df.iloc[2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e57e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1.similarity(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8434654b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723d08aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_articles = articles_list[similar_articles_idxs]\n",
    "pprint(similar_articles.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9eb754",
   "metadata": {},
   "source": [
    "Lastly, we will create create a mini-dashboard containing,\n",
    "1. a widget with all of our titles,\n",
    "2. a function with the steps we followed above,\n",
    "3. a panel object to store a title, the widget, and the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a841ebe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = df.title.unique().tolist()\n",
    "title_widget = pn.widgets.Select(value=choice(titles), options=titles, name='Articles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d0c2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pn.depends(title_widget.param.value)\n",
    "def article_recommender(title_widget):\n",
    "    \n",
    "    article_idx = np.where(articles_list == title_widget)[0][0]\n",
    "    article_similarities = doc_sim_df.iloc[article_idx].values\n",
    "    similar_title_idxs = np.argsort(-article_similarities)[1:6]\n",
    "    similar_titles = articles_list[similar_title_idxs]\n",
    "    \n",
    "    return pn.Column(*similar_titles, width=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60aaf35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = pn.pane.Markdown(f\"# Small Recommendation Engine\", style={\"color\": \"#000000\"}, width=600, height=50,\n",
    "                        sizing_mode=\"stretch_width\", margin=(10,10,10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b99728",
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.Column(text, title_widget, article_recommender, align='center', width=600, height=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69a8aff",
   "metadata": {},
   "source": [
    "## 6. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6770327e",
   "metadata": {},
   "source": [
    "Blind Spots\n",
    "\n",
    "With additional time we could have,\n",
    "1. Further tweak the parameters of the vectorizers and models;\n",
    "2. Create visualizations of the document similarity to find more interesting patters;\n",
    "3. Take the title of an article out of the body of the article to create a better, less biased representation of the words within a document;\n",
    "4. Using Pytorch's nn.CosineSimilarity would help a lot with increasing the efficiency of our recommendation system;\n",
    "5. There should have been a lemmatization step in the preprocessing stage.\n",
    "\n",
    "Takeaways,\n",
    "1. Recommendation systems are an example of unsupervised machine learning;\n",
    "2. Recommendation systems can be created with or without users behavioural data;\n",
    "3. Creating bags of words requires careful attention to the parameters;\n",
    "4. Where possible, showcase a model or system in a mini-dashboard or data visualization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
