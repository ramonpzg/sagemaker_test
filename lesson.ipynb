{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea674d59",
   "metadata": {},
   "source": [
    "# Intro to Natural Language Processing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f79a17c",
   "metadata": {},
   "source": [
    "> \"You shall know a word by the company it keeps.\" ~ John R. Firth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "3bef7a43-b8fa-46fa-ad0f-846b1b95e4b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<iframe width=\"768\" height=\"432\" src=\"https://miro.com/app/live-embed/uXjVOlC3sTw=/?moveToViewport=-1354,-1121,2108,1681&embedId=334819522676\" frameborder=\"0\" scrolling=\"no\" allowfullscreen></iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "\n",
    "<iframe width=\"768\" height=\"432\" src=\"https://miro.com/app/live-embed/uXjVOlC3sTw=/?moveToViewport=-1354,-1121,2108,1681&embedId=334819522676\" frameborder=\"0\" scrolling=\"no\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4086650d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57d76b4",
   "metadata": {
    "tags": []
   },
   "source": [
    "The goal of this short demo is to cover the process of preparing and transforming text data in order to build a similarity based recommender system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3812d3aa",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e068da",
   "metadata": {},
   "source": [
    "1. Libraries\n",
    "2. The Data\n",
    "3. Flash NLP Intro\n",
    "4. Cleaning\n",
    "5. Recommendation System\n",
    "6. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77dd8621",
   "metadata": {},
   "source": [
    "## 1. Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34491fee",
   "metadata": {},
   "source": [
    "Download the following libraries, if not available already. You can check with `!pip list` or with `!conda list` in a new cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0e7ea5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install -U spacy panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "970a5d4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, js_modules, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "    if (js_modules == null) js_modules = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls.length === 0 && js_modules.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length + js_modules.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error() {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < css_urls.length; i++) {\n",
       "      var url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    var skip = [];\n",
       "    if (window.requirejs) {\n",
       "      window.requirejs.config({'paths': {'tabulator': 'https://unpkg.com/tabulator-tables@4.9.3/dist/js/tabulator'}});\n",
       "      require([], function() {\n",
       "      })\n",
       "    }\n",
       "    if (((window['tabulator'] !== undefined) && (!(window['tabulator'] instanceof HTMLElement))) || window.requirejs) {\n",
       "      var urls = ['https://unpkg.com/tabulator-tables@4.9.3/dist/js/tabulator.js', 'https://unpkg.com/moment@2.27.0/moment.js'];\n",
       "      for (var i = 0; i < urls.length; i++) {\n",
       "        skip.push(urls[i])\n",
       "      }\n",
       "    }\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      if (skip.indexOf(url) >= 0) { on_load(); continue; }\n",
       "      var element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "    for (var i = 0; i < js_modules.length; i++) {\n",
       "      var url = js_modules[i];\n",
       "      if (skip.indexOf(url) >= 0) { on_load(); continue; }\n",
       "      var element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      element.type = \"module\";\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "    if (!js_urls.length && !js_modules.length) {\n",
       "      on_load()\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://unpkg.com/tabulator-tables@4.9.3/dist/js/tabulator.js\", \"https://unpkg.com/moment@2.27.0/moment.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-2.3.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.3.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.3.3.min.js\", \"https://unpkg.com/@holoviz/panel@0.11.3/dist/panel.min.js\"];\n",
       "  var js_modules = [];\n",
       "  var css_urls = [\"https://unpkg.com/tabulator-tables@4.9.3/dist/css/tabulator_simple.min.css\", \"https://unpkg.com/@holoviz/panel@0.11.3/dist/css/widgets.css\", \"https://unpkg.com/@holoviz/panel@0.11.3/dist/css/card.css\", \"https://unpkg.com/@holoviz/panel@0.11.3/dist/css/json.css\", \"https://unpkg.com/@holoviz/panel@0.11.3/dist/css/dataframe.css\", \"https://unpkg.com/@holoviz/panel@0.11.3/dist/css/loading.css\", \"https://unpkg.com/@holoviz/panel@0.11.3/dist/css/markdown.css\", \"https://unpkg.com/@holoviz/panel@0.11.3/dist/css/alerts.css\"];\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      inject_raw_css(\"\\n    .bk.pn-loading.arcs:before {\\n      background-image: url(\\\"data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiBzdHlsZT0ibWFyZ2luOiBhdXRvOyBiYWNrZ3JvdW5kOiBub25lOyBkaXNwbGF5OiBibG9jazsgc2hhcGUtcmVuZGVyaW5nOiBhdXRvOyIgdmlld0JveD0iMCAwIDEwMCAxMDAiIHByZXNlcnZlQXNwZWN0UmF0aW89InhNaWRZTWlkIj4gIDxjaXJjbGUgY3g9IjUwIiBjeT0iNTAiIHI9IjMyIiBzdHJva2Utd2lkdGg9IjgiIHN0cm9rZT0iI2MzYzNjMyIgc3Ryb2tlLWRhc2hhcnJheT0iNTAuMjY1NDgyNDU3NDM2NjkgNTAuMjY1NDgyNDU3NDM2NjkiIGZpbGw9Im5vbmUiIHN0cm9rZS1saW5lY2FwPSJyb3VuZCI+ICAgIDxhbmltYXRlVHJhbnNmb3JtIGF0dHJpYnV0ZU5hbWU9InRyYW5zZm9ybSIgdHlwZT0icm90YXRlIiByZXBlYXRDb3VudD0iaW5kZWZpbml0ZSIgZHVyPSIxcyIga2V5VGltZXM9IjA7MSIgdmFsdWVzPSIwIDUwIDUwOzM2MCA1MCA1MCI+PC9hbmltYXRlVHJhbnNmb3JtPiAgPC9jaXJjbGU+PC9zdmc+\\\")\\n    }\\n    \");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    function(Bokeh) {} // ensure no trailing comma for IE\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    }\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, js_modules, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.holoviews_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, js_modules, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n    if (js_modules == null) js_modules = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls.length === 0 && js_modules.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length + js_modules.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    var skip = [];\n    if (window.requirejs) {\n      window.requirejs.config({'paths': {'tabulator': 'https://unpkg.com/tabulator-tables@4.9.3/dist/js/tabulator'}});\n      require([], function() {\n      })\n    }\n    if (((window['tabulator'] !== undefined) && (!(window['tabulator'] instanceof HTMLElement))) || window.requirejs) {\n      var urls = ['https://unpkg.com/tabulator-tables@4.9.3/dist/js/tabulator.js', 'https://unpkg.com/moment@2.27.0/moment.js'];\n      for (var i = 0; i < urls.length; i++) {\n        skip.push(urls[i])\n      }\n    }\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      if (skip.indexOf(url) >= 0) { on_load(); continue; }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (var i = 0; i < js_modules.length; i++) {\n      var url = js_modules[i];\n      if (skip.indexOf(url) >= 0) { on_load(); continue; }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    if (!js_urls.length && !js_modules.length) {\n      on_load()\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  var js_urls = [\"https://unpkg.com/tabulator-tables@4.9.3/dist/js/tabulator.js\", \"https://unpkg.com/moment@2.27.0/moment.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-2.3.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.3.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.3.3.min.js\", \"https://unpkg.com/@holoviz/panel@0.11.3/dist/panel.min.js\"];\n  var js_modules = [];\n  var css_urls = [\"https://unpkg.com/tabulator-tables@4.9.3/dist/css/tabulator_simple.min.css\", \"https://unpkg.com/@holoviz/panel@0.11.3/dist/css/widgets.css\", \"https://unpkg.com/@holoviz/panel@0.11.3/dist/css/card.css\", \"https://unpkg.com/@holoviz/panel@0.11.3/dist/css/json.css\", \"https://unpkg.com/@holoviz/panel@0.11.3/dist/css/dataframe.css\", \"https://unpkg.com/@holoviz/panel@0.11.3/dist/css/loading.css\", \"https://unpkg.com/@holoviz/panel@0.11.3/dist/css/markdown.css\", \"https://unpkg.com/@holoviz/panel@0.11.3/dist/css/alerts.css\"];\n  var inline_js = [\n    function(Bokeh) {\n      inject_raw_css(\"\\n    .bk.pn-loading.arcs:before {\\n      background-image: url(\\\"data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiBzdHlsZT0ibWFyZ2luOiBhdXRvOyBiYWNrZ3JvdW5kOiBub25lOyBkaXNwbGF5OiBibG9jazsgc2hhcGUtcmVuZGVyaW5nOiBhdXRvOyIgdmlld0JveD0iMCAwIDEwMCAxMDAiIHByZXNlcnZlQXNwZWN0UmF0aW89InhNaWRZTWlkIj4gIDxjaXJjbGUgY3g9IjUwIiBjeT0iNTAiIHI9IjMyIiBzdHJva2Utd2lkdGg9IjgiIHN0cm9rZT0iI2MzYzNjMyIgc3Ryb2tlLWRhc2hhcnJheT0iNTAuMjY1NDgyNDU3NDM2NjkgNTAuMjY1NDgyNDU3NDM2NjkiIGZpbGw9Im5vbmUiIHN0cm9rZS1saW5lY2FwPSJyb3VuZCI+ICAgIDxhbmltYXRlVHJhbnNmb3JtIGF0dHJpYnV0ZU5hbWU9InRyYW5zZm9ybSIgdHlwZT0icm90YXRlIiByZXBlYXRDb3VudD0iaW5kZWZpbml0ZSIgZHVyPSIxcyIga2V5VGltZXM9IjA7MSIgdmFsdWVzPSIwIDUwIDUwOzM2MCA1MCA1MCI+PC9hbmltYXRlVHJhbnNmb3JtPiAgPC9jaXJjbGU+PC9zdmc+\\\")\\n    }\\n    \");\n    },\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    function(Bokeh) {} // ensure no trailing comma for IE\n  ];\n\n  function run_inline_js() {\n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, js_modules, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "if ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n",
       "  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n",
       "}\n",
       "\n",
       "\n",
       "    function JupyterCommManager() {\n",
       "    }\n",
       "\n",
       "    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n",
       "      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n",
       "        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n",
       "        comm_manager.register_target(comm_id, function(comm) {\n",
       "          comm.on_msg(msg_handler);\n",
       "        });\n",
       "      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n",
       "        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n",
       "          comm.onMsg = msg_handler;\n",
       "        });\n",
       "      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n",
       "        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n",
       "          var messages = comm.messages[Symbol.asyncIterator]();\n",
       "          function processIteratorResult(result) {\n",
       "            var message = result.value;\n",
       "            console.log(message)\n",
       "            var content = {data: message.data, comm_id};\n",
       "            var buffers = []\n",
       "            for (var buffer of message.buffers || []) {\n",
       "              buffers.push(new DataView(buffer))\n",
       "            }\n",
       "            var metadata = message.metadata || {};\n",
       "            var msg = {content, buffers, metadata}\n",
       "            msg_handler(msg);\n",
       "            return messages.next().then(processIteratorResult);\n",
       "          }\n",
       "          return messages.next().then(processIteratorResult);\n",
       "        })\n",
       "      }\n",
       "    }\n",
       "\n",
       "    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n",
       "      if (comm_id in window.PyViz.comms) {\n",
       "        return window.PyViz.comms[comm_id];\n",
       "      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n",
       "        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n",
       "        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n",
       "        if (msg_handler) {\n",
       "          comm.on_msg(msg_handler);\n",
       "        }\n",
       "      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n",
       "        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n",
       "        comm.open();\n",
       "        if (msg_handler) {\n",
       "          comm.onMsg = msg_handler;\n",
       "        }\n",
       "      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n",
       "        var comm_promise = google.colab.kernel.comms.open(comm_id)\n",
       "        comm_promise.then((comm) => {\n",
       "          window.PyViz.comms[comm_id] = comm;\n",
       "          if (msg_handler) {\n",
       "            var messages = comm.messages[Symbol.asyncIterator]();\n",
       "            function processIteratorResult(result) {\n",
       "              var message = result.value;\n",
       "              var content = {data: message.data};\n",
       "              var metadata = message.metadata || {comm_id};\n",
       "              var msg = {content, metadata}\n",
       "              msg_handler(msg);\n",
       "              return messages.next().then(processIteratorResult);\n",
       "            }\n",
       "            return messages.next().then(processIteratorResult);\n",
       "          }\n",
       "        }) \n",
       "        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n",
       "          return comm_promise.then((comm) => {\n",
       "            comm.send(data, metadata, buffers, disposeOnDone);\n",
       "          });\n",
       "        };\n",
       "        var comm = {\n",
       "          send: sendClosure\n",
       "        };\n",
       "      }\n",
       "      window.PyViz.comms[comm_id] = comm;\n",
       "      return comm;\n",
       "    }\n",
       "    window.PyViz.comm_manager = new JupyterCommManager();\n",
       "    \n",
       "\n",
       "\n",
       "var JS_MIME_TYPE = 'application/javascript';\n",
       "var HTML_MIME_TYPE = 'text/html';\n",
       "var EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\n",
       "var CLASS_NAME = 'output';\n",
       "\n",
       "/**\n",
       " * Render data to the DOM node\n",
       " */\n",
       "function render(props, node) {\n",
       "  var div = document.createElement(\"div\");\n",
       "  var script = document.createElement(\"script\");\n",
       "  node.appendChild(div);\n",
       "  node.appendChild(script);\n",
       "}\n",
       "\n",
       "/**\n",
       " * Handle when a new output is added\n",
       " */\n",
       "function handle_add_output(event, handle) {\n",
       "  var output_area = handle.output_area;\n",
       "  var output = handle.output;\n",
       "  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "    return\n",
       "  }\n",
       "  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "  if (id !== undefined) {\n",
       "    var nchildren = toinsert.length;\n",
       "    var html_node = toinsert[nchildren-1].children[0];\n",
       "    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "    var scripts = [];\n",
       "    var nodelist = html_node.querySelectorAll(\"script\");\n",
       "    for (var i in nodelist) {\n",
       "      if (nodelist.hasOwnProperty(i)) {\n",
       "        scripts.push(nodelist[i])\n",
       "      }\n",
       "    }\n",
       "\n",
       "    scripts.forEach( function (oldScript) {\n",
       "      var newScript = document.createElement(\"script\");\n",
       "      var attrs = [];\n",
       "      var nodemap = oldScript.attributes;\n",
       "      for (var j in nodemap) {\n",
       "        if (nodemap.hasOwnProperty(j)) {\n",
       "          attrs.push(nodemap[j])\n",
       "        }\n",
       "      }\n",
       "      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n",
       "      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n",
       "      oldScript.parentNode.replaceChild(newScript, oldScript);\n",
       "    });\n",
       "    if (JS_MIME_TYPE in output.data) {\n",
       "      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n",
       "    }\n",
       "    output_area._hv_plot_id = id;\n",
       "    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n",
       "      window.PyViz.plot_index[id] = Bokeh.index[id];\n",
       "    } else {\n",
       "      window.PyViz.plot_index[id] = null;\n",
       "    }\n",
       "  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "    var bk_div = document.createElement(\"div\");\n",
       "    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "    var script_attrs = bk_div.children[0].attributes;\n",
       "    for (var i = 0; i < script_attrs.length; i++) {\n",
       "      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "    }\n",
       "    // store reference to server id on output_area\n",
       "    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "  }\n",
       "}\n",
       "\n",
       "/**\n",
       " * Handle when an output is cleared or removed\n",
       " */\n",
       "function handle_clear_output(event, handle) {\n",
       "  var id = handle.cell.output_area._hv_plot_id;\n",
       "  var server_id = handle.cell.output_area._bokeh_server_id;\n",
       "  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n",
       "  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n",
       "  if (server_id !== null) {\n",
       "    comm.send({event_type: 'server_delete', 'id': server_id});\n",
       "    return;\n",
       "  } else if (comm !== null) {\n",
       "    comm.send({event_type: 'delete', 'id': id});\n",
       "  }\n",
       "  delete PyViz.plot_index[id];\n",
       "  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n",
       "    var doc = window.Bokeh.index[id].model.document\n",
       "    doc.clear();\n",
       "    const i = window.Bokeh.documents.indexOf(doc);\n",
       "    if (i > -1) {\n",
       "      window.Bokeh.documents.splice(i, 1);\n",
       "    }\n",
       "  }\n",
       "}\n",
       "\n",
       "/**\n",
       " * Handle kernel restart event\n",
       " */\n",
       "function handle_kernel_cleanup(event, handle) {\n",
       "  delete PyViz.comms[\"hv-extension-comm\"];\n",
       "  window.PyViz.plot_index = {}\n",
       "}\n",
       "\n",
       "/**\n",
       " * Handle update_display_data messages\n",
       " */\n",
       "function handle_update_output(event, handle) {\n",
       "  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n",
       "  handle_add_output(event, handle)\n",
       "}\n",
       "\n",
       "function register_renderer(events, OutputArea) {\n",
       "  function append_mime(data, metadata, element) {\n",
       "    // create a DOM node to render to\n",
       "    var toinsert = this.create_output_subarea(\n",
       "    metadata,\n",
       "    CLASS_NAME,\n",
       "    EXEC_MIME_TYPE\n",
       "    );\n",
       "    this.keyboard_manager.register_events(toinsert);\n",
       "    // Render to node\n",
       "    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "    render(props, toinsert[0]);\n",
       "    element.append(toinsert);\n",
       "    return toinsert\n",
       "  }\n",
       "\n",
       "  events.on('output_added.OutputArea', handle_add_output);\n",
       "  events.on('output_updated.OutputArea', handle_update_output);\n",
       "  events.on('clear_output.CodeCell', handle_clear_output);\n",
       "  events.on('delete.Cell', handle_clear_output);\n",
       "  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n",
       "\n",
       "  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "    safe: true,\n",
       "    index: 0\n",
       "  });\n",
       "}\n",
       "\n",
       "if (window.Jupyter !== undefined) {\n",
       "  try {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  } catch(err) {\n",
       "  }\n",
       "}\n"
      ],
      "application/vnd.holoviews_load.v0+json": "\nif ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n}\n\n\n    function JupyterCommManager() {\n    }\n\n    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        comm_manager.register_target(comm_id, function(comm) {\n          comm.on_msg(msg_handler);\n        });\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n          comm.onMsg = msg_handler;\n        });\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n          var messages = comm.messages[Symbol.asyncIterator]();\n          function processIteratorResult(result) {\n            var message = result.value;\n            console.log(message)\n            var content = {data: message.data, comm_id};\n            var buffers = []\n            for (var buffer of message.buffers || []) {\n              buffers.push(new DataView(buffer))\n            }\n            var metadata = message.metadata || {};\n            var msg = {content, buffers, metadata}\n            msg_handler(msg);\n            return messages.next().then(processIteratorResult);\n          }\n          return messages.next().then(processIteratorResult);\n        })\n      }\n    }\n\n    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n      if (comm_id in window.PyViz.comms) {\n        return window.PyViz.comms[comm_id];\n      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n        if (msg_handler) {\n          comm.on_msg(msg_handler);\n        }\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n        comm.open();\n        if (msg_handler) {\n          comm.onMsg = msg_handler;\n        }\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        var comm_promise = google.colab.kernel.comms.open(comm_id)\n        comm_promise.then((comm) => {\n          window.PyViz.comms[comm_id] = comm;\n          if (msg_handler) {\n            var messages = comm.messages[Symbol.asyncIterator]();\n            function processIteratorResult(result) {\n              var message = result.value;\n              var content = {data: message.data};\n              var metadata = message.metadata || {comm_id};\n              var msg = {content, metadata}\n              msg_handler(msg);\n              return messages.next().then(processIteratorResult);\n            }\n            return messages.next().then(processIteratorResult);\n          }\n        }) \n        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n          return comm_promise.then((comm) => {\n            comm.send(data, metadata, buffers, disposeOnDone);\n          });\n        };\n        var comm = {\n          send: sendClosure\n        };\n      }\n      window.PyViz.comms[comm_id] = comm;\n      return comm;\n    }\n    window.PyViz.comm_manager = new JupyterCommManager();\n    \n\n\nvar JS_MIME_TYPE = 'application/javascript';\nvar HTML_MIME_TYPE = 'text/html';\nvar EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\nvar CLASS_NAME = 'output';\n\n/**\n * Render data to the DOM node\n */\nfunction render(props, node) {\n  var div = document.createElement(\"div\");\n  var script = document.createElement(\"script\");\n  node.appendChild(div);\n  node.appendChild(script);\n}\n\n/**\n * Handle when a new output is added\n */\nfunction handle_add_output(event, handle) {\n  var output_area = handle.output_area;\n  var output = handle.output;\n  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n    return\n  }\n  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n  if (id !== undefined) {\n    var nchildren = toinsert.length;\n    var html_node = toinsert[nchildren-1].children[0];\n    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n    var scripts = [];\n    var nodelist = html_node.querySelectorAll(\"script\");\n    for (var i in nodelist) {\n      if (nodelist.hasOwnProperty(i)) {\n        scripts.push(nodelist[i])\n      }\n    }\n\n    scripts.forEach( function (oldScript) {\n      var newScript = document.createElement(\"script\");\n      var attrs = [];\n      var nodemap = oldScript.attributes;\n      for (var j in nodemap) {\n        if (nodemap.hasOwnProperty(j)) {\n          attrs.push(nodemap[j])\n        }\n      }\n      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n      oldScript.parentNode.replaceChild(newScript, oldScript);\n    });\n    if (JS_MIME_TYPE in output.data) {\n      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n    }\n    output_area._hv_plot_id = id;\n    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n      window.PyViz.plot_index[id] = Bokeh.index[id];\n    } else {\n      window.PyViz.plot_index[id] = null;\n    }\n  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n    var bk_div = document.createElement(\"div\");\n    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n    var script_attrs = bk_div.children[0].attributes;\n    for (var i = 0; i < script_attrs.length; i++) {\n      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n    }\n    // store reference to server id on output_area\n    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n  }\n}\n\n/**\n * Handle when an output is cleared or removed\n */\nfunction handle_clear_output(event, handle) {\n  var id = handle.cell.output_area._hv_plot_id;\n  var server_id = handle.cell.output_area._bokeh_server_id;\n  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n  if (server_id !== null) {\n    comm.send({event_type: 'server_delete', 'id': server_id});\n    return;\n  } else if (comm !== null) {\n    comm.send({event_type: 'delete', 'id': id});\n  }\n  delete PyViz.plot_index[id];\n  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n    var doc = window.Bokeh.index[id].model.document\n    doc.clear();\n    const i = window.Bokeh.documents.indexOf(doc);\n    if (i > -1) {\n      window.Bokeh.documents.splice(i, 1);\n    }\n  }\n}\n\n/**\n * Handle kernel restart event\n */\nfunction handle_kernel_cleanup(event, handle) {\n  delete PyViz.comms[\"hv-extension-comm\"];\n  window.PyViz.plot_index = {}\n}\n\n/**\n * Handle update_display_data messages\n */\nfunction handle_update_output(event, handle) {\n  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n  handle_add_output(event, handle)\n}\n\nfunction register_renderer(events, OutputArea) {\n  function append_mime(data, metadata, element) {\n    // create a DOM node to render to\n    var toinsert = this.create_output_subarea(\n    metadata,\n    CLASS_NAME,\n    EXEC_MIME_TYPE\n    );\n    this.keyboard_manager.register_events(toinsert);\n    // Render to node\n    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n    render(props, toinsert[0]);\n    element.append(toinsert);\n    return toinsert\n  }\n\n  events.on('output_added.OutputArea', handle_add_output);\n  events.on('output_updated.OutputArea', handle_update_output);\n  events.on('clear_output.CodeCell', handle_clear_output);\n  events.on('delete.Cell', handle_clear_output);\n  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n\n  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n    safe: true,\n    index: 0\n  });\n}\n\nif (window.Jupyter !== undefined) {\n  try {\n    var events = require('base/js/events');\n    var OutputArea = require('notebook/js/outputarea').OutputArea;\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  } catch(err) {\n  }\n}\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import json, re, spacy\n",
    "from random import choice\n",
    "import pandas as pd, numpy as np\n",
    "from pprint import pprint\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import panel as pn\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "pn.extension()\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc67fb6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46249716",
   "metadata": {},
   "source": [
    "With have been given a random corpus of news articles, plus some additional information (outlined below), and we want to make a useful product with it.\n",
    "\n",
    "| Column | Content |\n",
    "|--------|---------|\n",
    "|title |Title of article|\n",
    "|text | Text inside article|\n",
    "|domain | Domain Url of article|\n",
    "|date | YYYY-MM-DD Time|\n",
    "|description | Abstract of article|\n",
    "|url | Url of article|\n",
    "|image_url | Image if available|\n",
    "\n",
    "Here is the full description of the dataset from HugginFace.\n",
    "\n",
    "> \"CC-News dataset contains news articles from news sites all over the world. The data is available on AWS S3 in the Common Crawl bucket at /crawl-data/CC-NEWS/. This version of the dataset has been prepared using news-please - an integrated web crawler and information extractor for news.\n",
    "It contains 708241 English language news articles published between Jan 2017 and December 2019. It represents a small portion of the English language subset of the CC-News dataset.\" ~ [Hugging Face cc_news](https://huggingface.co/datasets/cc_news)\n",
    "\n",
    "Before we do any data cleaning, let's read in the data and explore it a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "133caa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"cc_news_sample.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f4844a",
   "metadata": {},
   "source": [
    "Let's see how many articles we have and then examine the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "f2e9547f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 7)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "08ceaea9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>domain</th>\n",
       "      <th>date</th>\n",
       "      <th>description</th>\n",
       "      <th>url</th>\n",
       "      <th>image_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cristiano Ronaldo banned for 5 games after pus...</td>\n",
       "      <td>(AP Photo/Manu Fernandez). Real Madrid's Crist...</td>\n",
       "      <td>www.wave3.com</td>\n",
       "      <td>2017-08-14 00:00:00</td>\n",
       "      <td></td>\n",
       "      <td>http://www.wave3.com/story/36129496/cristiano-...</td>\n",
       "      <td>http://APMOBILE.images.worldnow.com/images/146...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why the Philippines is not truly independent</td>\n",
       "      <td>FOR leftists, American assistance in the Maraw...</td>\n",
       "      <td>www.manilatimes.net</td>\n",
       "      <td>2017-06-15 00:21:39</td>\n",
       "      <td></td>\n",
       "      <td>http://www.manilatimes.net/philippines-not-tru...</td>\n",
       "      <td>http://manilatimes.net/wp-content/uploads/2016...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tyron Charles death: Murder victim 'dumped in ...</td>\n",
       "      <td>Image copyright Family handout Image caption T...</td>\n",
       "      <td>www.bbc.com</td>\n",
       "      <td>2018-07-04 20:09:54</td>\n",
       "      <td>Police secretly recorded prison visits to find...</td>\n",
       "      <td>https://www.bbc.com/news/uk-england-leeds-4471...</td>\n",
       "      <td>https://ichef.bbci.co.uk/news/1024/branded_new...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>North Korean official accuses U.S. of turning ...</td>\n",
       "      <td>Justice Neil Gorsuch heard his first arguments...</td>\n",
       "      <td>theweek.com</td>\n",
       "      <td>2017-04-17 19:12:38</td>\n",
       "      <td>Official site of The Week Magazine, offering c...</td>\n",
       "      <td>http://theweek.com/speedreads/692792/north-kor...</td>\n",
       "      <td>http://api.theweek.com/sites/default/files/sty...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Spring's Sweet Start: Dairy Queen's Free Cone Day</td>\n",
       "      <td>What to Know Tuesday, March 20\\nParticipating ...</td>\n",
       "      <td>www.nbclosangeles.com</td>\n",
       "      <td>2018-03-19 10:44:02</td>\n",
       "      <td>Find your pay-nothing small vanilla cone on th...</td>\n",
       "      <td>https://www.nbclosangeles.com/news/local/Sprin...</td>\n",
       "      <td>https://media.nbclosangeles.com/images/1200*67...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Cristiano Ronaldo banned for 5 games after pus...   \n",
       "1       Why the Philippines is not truly independent   \n",
       "2  Tyron Charles death: Murder victim 'dumped in ...   \n",
       "3  North Korean official accuses U.S. of turning ...   \n",
       "4  Spring's Sweet Start: Dairy Queen's Free Cone Day   \n",
       "\n",
       "                                                text                 domain  \\\n",
       "0  (AP Photo/Manu Fernandez). Real Madrid's Crist...          www.wave3.com   \n",
       "1  FOR leftists, American assistance in the Maraw...    www.manilatimes.net   \n",
       "2  Image copyright Family handout Image caption T...            www.bbc.com   \n",
       "3  Justice Neil Gorsuch heard his first arguments...            theweek.com   \n",
       "4  What to Know Tuesday, March 20\\nParticipating ...  www.nbclosangeles.com   \n",
       "\n",
       "                  date                                        description  \\\n",
       "0  2017-08-14 00:00:00                                                      \n",
       "1  2017-06-15 00:21:39                                                      \n",
       "2  2018-07-04 20:09:54  Police secretly recorded prison visits to find...   \n",
       "3  2017-04-17 19:12:38  Official site of The Week Magazine, offering c...   \n",
       "4  2018-03-19 10:44:02  Find your pay-nothing small vanilla cone on th...   \n",
       "\n",
       "                                                 url  \\\n",
       "0  http://www.wave3.com/story/36129496/cristiano-...   \n",
       "1  http://www.manilatimes.net/philippines-not-tru...   \n",
       "2  https://www.bbc.com/news/uk-england-leeds-4471...   \n",
       "3  http://theweek.com/speedreads/692792/north-kor...   \n",
       "4  https://www.nbclosangeles.com/news/local/Sprin...   \n",
       "\n",
       "                                           image_url  \n",
       "0  http://APMOBILE.images.worldnow.com/images/146...  \n",
       "1  http://manilatimes.net/wp-content/uploads/2016...  \n",
       "2  https://ichef.bbci.co.uk/news/1024/branded_new...  \n",
       "3  http://api.theweek.com/sites/default/files/sty...  \n",
       "4  https://media.nbclosangeles.com/images/1200*67...  "
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eaacefe",
   "metadata": {},
   "source": [
    "## 3. Flash NLP Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ace9fa",
   "metadata": {},
   "source": [
    "Let's pick a random article using `.loc[index, column]` on our dataframe and let's examine it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "539bb68f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('British foreign secretary Boris Johnson has denounced Moscow’s denials of '\n",
      " 'involvement in the nerve weapon attack on a former Russian double agent as '\n",
      " '“increasingly absurd”.\\n'\n",
      " 'Mr Johnson’s remarks came as he received further support from the European '\n",
      " 'Union and Nato on Monday over the attack.\\n'\n",
      " 'The secretary of state for foreign affairs discussed the attempted murder of '\n",
      " 'Sergei Skripal and his daughter Yulia at a session of EU foreign ministers, '\n",
      " 'which he addressed. Mr Johnson also discussed the incident in a meeting with '\n",
      " 'Nato secretary-general Jens Stoltenberg after statements of solidarity last '\n",
      " 'week by a host of Western governments.\\n'\n",
      " '“Our Nato allies have shown such undivided support,” said Mr Johnson after '\n",
      " 'meeting Mr Stoltenberg.\\n'\n",
      " 'All 28 EU foreign ministers issued a joint statement on the attack, '\n",
      " 'expressing “unqualified solidarity”.\\n'\n",
      " '“The European Union takes extremely seriously the UK government’s assessment '\n",
      " 'that it is highly likely that the Russian Federation is responsible,” the '\n",
      " 'statement noted.\\n'\n",
      " 'Russia, including president Vladimir Putin, denies any involvement.\\n'\n",
      " 'Mr Johnson said he saw a “classic Russian strategy” as he arrived for the EU '\n",
      " 'meeting, a day after Putin was re-elected for another six-year term as '\n",
      " 'president.\\n'\n",
      " '“They’re not fooling anybody anymore,” said Mr Johnson. “There is scarcely a '\n",
      " 'country around the table here in Brussels that has not been affected in '\n",
      " 'recent years by some kind of malign or disruptive Russian behaviour.”\\n'\n",
      " 'Mr Stoltenberg echoed that position: “Russia will continue to seek to divide '\n",
      " 'us.”\\n'\n",
      " 'Swedish foreign minister Margot Wallstrom, meanwhile, accused the Russian '\n",
      " 'foreign ministry of deliberately sowing confusion by suggesting the nerve '\n",
      " 'agent used in the attack on the Skripals might have come from Sweden.\\n'\n",
      " '“This is just ridiculous and totally unfounded,” said Ms Wallstrom. “I think '\n",
      " 'they are trying to divert the real issues here.”\\n'\n",
      " 'Moscow on Saturday announced the expulsion of 23 British diplomats in a '\n",
      " 'response to Britain’s decision last week to expel the same number of Russian '\n",
      " 'diplomats from London.\\n'\n",
      " 'On Sunday, Mr Johnson accused Russia of stockpiling the deadly Soviet-era '\n",
      " 'nerve agent Novichok used to poison the Skripals, a charge Moscow denies.\\n'\n",
      " 'The couple were found unconscious on a bench in the English city of '\n",
      " 'Salisbury on March 4th and remain in a critical condition in hospital.\\n'\n",
      " 'Asked what support Britain had requested from the EU and Nato to counter '\n",
      " 'Russia, Mr Johnson replied that he was seeking to intensify work on a range '\n",
      " 'of strategies. These include defences against Russian attacks on computer '\n",
      " 'networks, challenging disinformation campaigns and acting against criminal '\n",
      " 'financial networks.\\n'\n",
      " '“There are things we can and must do together,” said Mr Johnson, adding that '\n",
      " 'Britain was “going after the money that has been illicitly or corruptly '\n",
      " 'obtained”, without giving details.\\n'\n",
      " 'While there was no immediate prospect of further sanctions on Russia, '\n",
      " 'British prime minister Theresa May will have an opportunity to present her '\n",
      " 'case for any such measures at an EU summit on Thursday, or call for others '\n",
      " 'to expel diplomats.\\n'\n",
      " '“We need to put pressure on Russia to take part in a real enquiry about the '\n",
      " 'attack,” said Belgian foreign minister Didier Reynders . – Reuters')\n"
     ]
    }
   ],
   "source": [
    "random_article = df.iloc[choice(range(5000)), 1]\n",
    "pprint(random_article)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e408ec",
   "metadata": {},
   "source": [
    "Notice how the review above looks a bit odd and it has a few characters that will not be useful for our analysis. Let's examine a cleaner version of the article above by running it through `spaCy`'s tokenizer.\n",
    "\n",
    "When we tokenize a document, we are separating all of its content into each of its components, i.e. words, numbers, punctiations and the like, to make it easier to process it, clean it, transform it and to run computations on it.\n",
    "\n",
    "For this part, we will load an english model, instantiate it and pass an example article through it. You may need to run the cell below first to download the english model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c55811",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "e78c1b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "ff4e249a",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_article = nlp(random_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "e106529c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "British foreign secretary Boris Johnson has denounced Moscow’s denials of involvement in the nerve weapon attack on a former Russian double agent as “increasingly absurd”.\n",
       "Mr Johnson’s remarks came as he received further support from the European Union and Nato on Monday over the attack.\n",
       "The secretary of state for foreign affairs discussed the attempted murder of Sergei Skripal and his daughter Yulia at a session of EU foreign ministers, which he addressed. Mr Johnson also discussed the incident in a meeting with Nato secretary-general Jens Stoltenberg after statements of solidarity last week by a host of Western governments.\n",
       "“Our Nato allies have shown such undivided support,” said Mr Johnson after meeting Mr Stoltenberg.\n",
       "All 28 EU foreign ministers issued a joint statement on the attack, expressing “unqualified solidarity”.\n",
       "“The European Union takes extremely seriously the UK government’s assessment that it is highly likely that the Russian Federation is responsible,” the statement noted.\n",
       "Russia, including president Vladimir Putin, denies any involvement.\n",
       "Mr Johnson said he saw a “classic Russian strategy” as he arrived for the EU meeting, a day after Putin was re-elected for another six-year term as president.\n",
       "“They’re not fooling anybody anymore,” said Mr Johnson. “There is scarcely a country around the table here in Brussels that has not been affected in recent years by some kind of malign or disruptive Russian behaviour.”\n",
       "Mr Stoltenberg echoed that position: “Russia will continue to seek to divide us.”\n",
       "Swedish foreign minister Margot Wallstrom, meanwhile, accused the Russian foreign ministry of deliberately sowing confusion by suggesting the nerve agent used in the attack on the Skripals might have come from Sweden.\n",
       "“This is just ridiculous and totally unfounded,” said Ms Wallstrom. “I think they are trying to divert the real issues here.”\n",
       "Moscow on Saturday announced the expulsion of 23 British diplomats in a response to Britain’s decision last week to expel the same number of Russian diplomats from London.\n",
       "On Sunday, Mr Johnson accused Russia of stockpiling the deadly Soviet-era nerve agent Novichok used to poison the Skripals, a charge Moscow denies.\n",
       "The couple were found unconscious on a bench in the English city of Salisbury on March 4th and remain in a critical condition in hospital.\n",
       "Asked what support Britain had requested from the EU and Nato to counter Russia, Mr Johnson replied that he was seeking to intensify work on a range of strategies. These include defences against Russian attacks on computer networks, challenging disinformation campaigns and acting against criminal financial networks.\n",
       "“There are things we can and must do together,” said Mr Johnson, adding that Britain was “going after the money that has been illicitly or corruptly obtained”, without giving details.\n",
       "While there was no immediate prospect of further sanctions on Russia, British prime minister Theresa May will have an opportunity to present her case for any such measures at an EU summit on Thursday, or call for others to expel diplomats.\n",
       "“We need to put pressure on Russia to take part in a real enquiry about the attack,” said Belgian foreign minister Didier Reynders . – Reuters"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_article"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a65af8f",
   "metadata": {},
   "source": [
    "Notice how much nicer our article looks now.\n",
    "\n",
    "We can also grab the sentences and view them one by one using the attribute `.sents` and the built in python function `next()`, since the attribute of a document that has been tokenized by spacy will always return an iterator. Conversely, we can add it to a loop and show each of the sentences in an article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "efbfb882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " British foreign secretary Boris Johnson has denounced Moscow’s denials of involvement in the nerve weapon attack on a former Russian double agent as “increasingly absurd”.)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(enumerate(parsed_article.sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "8f7d2e28",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence #0:\n",
      " British foreign secretary Boris Johnson has denounced Moscow’s denials of involvement in the nerve weapon attack on a former Russian double agent as “increasingly absurd”.\n",
      "\n",
      "Sentence #1:\n",
      " \n",
      "Mr Johnson’s remarks came as he received further support from the European Union and Nato on Monday over the attack.\n",
      "\n",
      "Sentence #2:\n",
      " \n",
      "The secretary of state for foreign affairs discussed the attempted murder of Sergei Skripal and his daughter Yulia at a session of EU foreign ministers, which he addressed.\n",
      "\n",
      "Sentence #3:\n",
      " Mr Johnson also discussed the incident in a meeting with Nato secretary-general Jens Stoltenberg after statements of solidarity last week by a host of Western governments.\n",
      "\n",
      "Sentence #4:\n",
      " \n",
      "“Our Nato allies have shown such undivided support,” said Mr Johnson after meeting Mr Stoltenberg.\n",
      "\n",
      "Sentence #5:\n",
      " \n",
      "All 28 EU foreign ministers issued a joint statement on the attack, expressing “unqualified solidarity”.\n",
      "\n",
      "Sentence #6:\n",
      " \n",
      "“The European Union takes extremely seriously the UK government’s assessment that it is highly likely that the Russian Federation is responsible,” the statement noted.\n",
      "\n",
      "Sentence #7:\n",
      " \n",
      "Russia, including president Vladimir Putin, denies any involvement.\n",
      "\n",
      "Sentence #8:\n",
      " \n",
      "Mr Johnson said he saw a “classic Russian strategy” as he arrived for the EU meeting, a day after Putin was re-elected for another six-year term as president.\n",
      "\n",
      "Sentence #9:\n",
      " \n",
      "“They’re not fooling anybody anymore,” said Mr Johnson.\n",
      "\n",
      "Sentence #10:\n",
      " “There is scarcely a country around the table here in Brussels that has not been affected in recent years by some kind of malign or disruptive Russian behaviour.”\n",
      "\n",
      "Sentence #11:\n",
      " \n",
      "Mr Stoltenberg echoed that position: “Russia will continue to seek to divide us.”\n",
      "\n",
      "Sentence #12:\n",
      " \n",
      "Swedish foreign minister Margot Wallstrom, meanwhile, accused the Russian foreign ministry of deliberately sowing confusion by suggesting the nerve agent used in the attack on the Skripals might have come from Sweden.\n",
      "\n",
      "Sentence #13:\n",
      " \n",
      "“This is just ridiculous and totally unfounded,” said Ms Wallstrom.\n",
      "\n",
      "Sentence #14:\n",
      " “I think they are trying to divert the real issues here.”\n",
      "\n",
      "Sentence #15:\n",
      " \n",
      "Moscow on Saturday announced the expulsion of 23 British diplomats in a response to Britain’s decision last week to expel the same number of Russian diplomats from London.\n",
      "\n",
      "Sentence #16:\n",
      " \n",
      "On Sunday, Mr Johnson accused Russia of stockpiling the deadly Soviet-era nerve agent Novichok used to poison the Skripals, a charge Moscow denies.\n",
      "\n",
      "Sentence #17:\n",
      " \n",
      "The couple were found unconscious on a bench in the English city of Salisbury on March 4th and remain in a critical condition in hospital.\n",
      "\n",
      "Sentence #18:\n",
      " \n",
      "Asked what support Britain had requested from the EU and Nato to counter Russia, Mr Johnson replied that he was seeking to intensify work on a range of strategies.\n",
      "\n",
      "Sentence #19:\n",
      " These include defences against Russian attacks on computer networks, challenging disinformation campaigns and acting against criminal financial networks.\n",
      "\n",
      "Sentence #20:\n",
      " \n",
      "“There are things we can and must do together,” said Mr Johnson, adding that Britain was “going after the money that has been illicitly or corruptly obtained”, without giving details.\n",
      "\n",
      "Sentence #21:\n",
      " \n",
      "While there was no immediate prospect of further sanctions on Russia, British prime minister Theresa May will have an opportunity to present her case for any such measures at an EU summit on Thursday, or call for others to expel diplomats.\n",
      "\n",
      "Sentence #22:\n",
      " \n",
      "“We need to put pressure on Russia to take part in a real enquiry about the attack,” said Belgian foreign minister Didier Reynders .\n",
      "\n",
      "Sentence #23:\n",
      " – Reuters\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for num, sentence in enumerate(parsed_article.sents):\n",
    "    print(f\"Sentence #{num}:\\n {sentence}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc73198",
   "metadata": {},
   "source": [
    "We can also have a look at the different kinds of entities in an article. These entities can be a person (called PERSON), and number (called CARDINAL), a geopolitical entity (called GPE), etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "2b3856d5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity #0: British -- NORP\n",
      "\n",
      "Entity #1: Boris Johnson -- PERSON\n",
      "\n",
      "Entity #2: Moscow -- GPE\n",
      "\n",
      "Entity #3: Russian -- NORP\n",
      "\n",
      "Entity #4: Johnson -- PERSON\n",
      "\n",
      "Entity #5: the European Union -- ORG\n",
      "\n",
      "Entity #6: Nato -- ORG\n",
      "\n",
      "Entity #7: Monday -- DATE\n",
      "\n",
      "Entity #8: Sergei Skripal -- PERSON\n",
      "\n",
      "Entity #9: Yulia -- PERSON\n",
      "\n",
      "Entity #10: EU -- ORG\n",
      "\n",
      "Entity #11: Johnson -- PERSON\n",
      "\n",
      "Entity #12: Nato -- ORG\n",
      "\n",
      "Entity #13: Jens Stoltenberg -- PERSON\n",
      "\n",
      "Entity #14: solidarity -- ORG\n",
      "\n",
      "Entity #15: last week -- DATE\n",
      "\n",
      "Entity #16: Western -- NORP\n",
      "\n",
      "Entity #17: Nato -- ORG\n",
      "\n",
      "Entity #18: Johnson -- PERSON\n",
      "\n",
      "Entity #19: Stoltenberg -- PERSON\n",
      "\n",
      "Entity #20: 28 -- CARDINAL\n",
      "\n",
      "Entity #21: EU -- ORG\n",
      "\n",
      "Entity #22: The European Union -- ORG\n",
      "\n",
      "Entity #23: UK -- GPE\n",
      "\n",
      "Entity #24: the Russian Federation -- GPE\n",
      "\n",
      "Entity #25: Russia -- GPE\n",
      "\n",
      "Entity #26: Vladimir Putin -- PERSON\n",
      "\n",
      "Entity #27: Johnson -- PERSON\n",
      "\n",
      "Entity #28: Russian -- NORP\n",
      "\n",
      "Entity #29: EU -- ORG\n",
      "\n",
      "Entity #30: a day -- DATE\n",
      "\n",
      "Entity #31: Putin -- PERSON\n",
      "\n",
      "Entity #32: six-year -- DATE\n",
      "\n",
      "Entity #33: Johnson -- PERSON\n",
      "\n",
      "Entity #34: Brussels -- GPE\n",
      "\n",
      "Entity #35: recent years -- DATE\n",
      "\n",
      "Entity #36: Russian -- NORP\n",
      "\n",
      "Entity #37: Stoltenberg -- PERSON\n",
      "\n",
      "Entity #38: Russia -- GPE\n",
      "\n",
      "Entity #39: Swedish -- NORP\n",
      "\n",
      "Entity #40: Margot Wallstrom -- PERSON\n",
      "\n",
      "Entity #41: Russian -- NORP\n",
      "\n",
      "Entity #42: Skripals -- ORG\n",
      "\n",
      "Entity #43: Sweden -- GPE\n",
      "\n",
      "Entity #44: Ms Wallstrom -- PERSON\n",
      "\n",
      "Entity #45: Moscow -- GPE\n",
      "\n",
      "Entity #46: Saturday -- DATE\n",
      "\n",
      "Entity #47: 23 -- CARDINAL\n",
      "\n",
      "Entity #48: British -- NORP\n",
      "\n",
      "Entity #49: Britain -- GPE\n",
      "\n",
      "Entity #50: last week -- DATE\n",
      "\n",
      "Entity #51: Russian -- NORP\n",
      "\n",
      "Entity #52: London -- GPE\n",
      "\n",
      "Entity #53: Sunday -- DATE\n",
      "\n",
      "Entity #54: Johnson -- PERSON\n",
      "\n",
      "Entity #55: Russia -- GPE\n",
      "\n",
      "Entity #56: Soviet -- NORP\n",
      "\n",
      "Entity #57: Novichok -- PERSON\n",
      "\n",
      "Entity #58: Skripals -- ORG\n",
      "\n",
      "Entity #59: Moscow -- GPE\n",
      "\n",
      "Entity #60: English -- LANGUAGE\n",
      "\n",
      "Entity #61: Salisbury -- GPE\n",
      "\n",
      "Entity #62: March 4th -- DATE\n",
      "\n",
      "Entity #63: Britain -- GPE\n",
      "\n",
      "Entity #64: EU -- ORG\n",
      "\n",
      "Entity #65: Nato -- ORG\n",
      "\n",
      "Entity #66: Russia -- GPE\n",
      "\n",
      "Entity #67: Johnson -- PERSON\n",
      "\n",
      "Entity #68: Russian -- NORP\n",
      "\n",
      "Entity #69: Johnson -- PERSON\n",
      "\n",
      "Entity #70: Britain -- GPE\n",
      "\n",
      "Entity #71: Russia -- GPE\n",
      "\n",
      "Entity #72: British -- NORP\n",
      "\n",
      "Entity #73: Theresa May -- PERSON\n",
      "\n",
      "Entity #74: EU -- ORG\n",
      "\n",
      "Entity #75: Thursday -- DATE\n",
      "\n",
      "Entity #76: Russia -- GPE\n",
      "\n",
      "Entity #77: Belgian -- NORP\n",
      "\n",
      "Entity #78: Didier Reynders -- PERSON\n",
      "\n",
      "Entity #79: Reuters -- ORG\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for num, entity in enumerate(parsed_article.ents):\n",
    "    print(f\"Entity #{num}: {entity} -- {entity.label_}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "e3bd5c58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Non-GPE locations, mountain ranges, bodies of water'"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain(\"LOC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac41a04",
   "metadata": {},
   "source": [
    "We can also check weather a word is a stopword or a punctuation, or we can even lemmatize our articles. Lemmatization is a way of taking the root of a word and bringing similar words to a common denominator, for example, `was` will become `be` and most plural words will become singular words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "1b5cf574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['British',\n",
       " 'foreign',\n",
       " 'secretary',\n",
       " 'Boris',\n",
       " 'Johnson',\n",
       " 'has',\n",
       " 'denounced',\n",
       " 'Moscow',\n",
       " '’s',\n",
       " 'denials']"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_list = []\n",
    "\n",
    "for token in parsed_article:\n",
    "    new_list.append(token.text)\n",
    "    \n",
    "    \n",
    "new_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "f2060f67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['British',\n",
       " 'foreign',\n",
       " 'secretary',\n",
       " 'Boris',\n",
       " 'Johnson',\n",
       " 'has',\n",
       " 'denounced',\n",
       " 'Moscow',\n",
       " '’s',\n",
       " 'denials']"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_list = [token.text for token in parsed_article]\n",
    "\n",
    "new_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "47510924",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original Text</th>\n",
       "      <th>Lemmatized Text</th>\n",
       "      <th>Punctuations</th>\n",
       "      <th>stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>British</td>\n",
       "      <td>british</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>foreign</td>\n",
       "      <td>foreign</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>secretary</td>\n",
       "      <td>secretary</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Boris</td>\n",
       "      <td>Boris</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Johnson</td>\n",
       "      <td>Johnson</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>has</td>\n",
       "      <td>have</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>denounced</td>\n",
       "      <td>denounce</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Moscow</td>\n",
       "      <td>Moscow</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>’s</td>\n",
       "      <td>’s</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>denials</td>\n",
       "      <td>denial</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>involvement</td>\n",
       "      <td>involvement</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>in</td>\n",
       "      <td>in</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>nerve</td>\n",
       "      <td>nerve</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>weapon</td>\n",
       "      <td>weapon</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>attack</td>\n",
       "      <td>attack</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>on</td>\n",
       "      <td>on</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>former</td>\n",
       "      <td>former</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Russian</td>\n",
       "      <td>russian</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>double</td>\n",
       "      <td>double</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>agent</td>\n",
       "      <td>agent</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>as</td>\n",
       "      <td>as</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>“</td>\n",
       "      <td>\"</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>increasingly</td>\n",
       "      <td>increasingly</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>absurd</td>\n",
       "      <td>absurd</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>”</td>\n",
       "      <td>\"</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>\\n</td>\n",
       "      <td>\\n</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Mr</td>\n",
       "      <td>Mr</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Johnson</td>\n",
       "      <td>Johnson</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>’s</td>\n",
       "      <td>’s</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>remarks</td>\n",
       "      <td>remark</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>came</td>\n",
       "      <td>come</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>as</td>\n",
       "      <td>as</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>he</td>\n",
       "      <td>he</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>received</td>\n",
       "      <td>receive</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>further</td>\n",
       "      <td>further</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>support</td>\n",
       "      <td>support</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>from</td>\n",
       "      <td>from</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>European</td>\n",
       "      <td>European</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Union</td>\n",
       "      <td>Union</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>and</td>\n",
       "      <td>and</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Nato</td>\n",
       "      <td>Nato</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>on</td>\n",
       "      <td>on</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Monday</td>\n",
       "      <td>Monday</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>over</td>\n",
       "      <td>over</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Original Text Lemmatized Text  Punctuations  stopwords\n",
       "0        British         british         False      False\n",
       "1        foreign         foreign         False      False\n",
       "2      secretary       secretary         False      False\n",
       "3          Boris           Boris         False      False\n",
       "4        Johnson         Johnson         False      False\n",
       "5            has            have         False       True\n",
       "6      denounced        denounce         False      False\n",
       "7         Moscow          Moscow         False      False\n",
       "8             ’s              ’s         False       True\n",
       "9        denials          denial         False      False\n",
       "10            of              of         False       True\n",
       "11   involvement     involvement         False      False\n",
       "12            in              in         False       True\n",
       "13           the             the         False       True\n",
       "14         nerve           nerve         False      False\n",
       "15        weapon          weapon         False      False\n",
       "16        attack          attack         False      False\n",
       "17            on              on         False       True\n",
       "18             a               a         False       True\n",
       "19        former          former         False       True\n",
       "20       Russian         russian         False      False\n",
       "21        double          double         False      False\n",
       "22         agent           agent         False      False\n",
       "23            as              as         False       True\n",
       "24             “               \"          True      False\n",
       "25  increasingly    increasingly         False      False\n",
       "26        absurd          absurd         False      False\n",
       "27             ”               \"          True      False\n",
       "28             .               .          True      False\n",
       "29            \\n              \\n         False      False\n",
       "30            Mr              Mr         False      False\n",
       "31       Johnson         Johnson         False      False\n",
       "32            ’s              ’s         False       True\n",
       "33       remarks          remark         False      False\n",
       "34          came            come         False      False\n",
       "35            as              as         False       True\n",
       "36            he              he         False       True\n",
       "37      received         receive         False      False\n",
       "38       further         further         False       True\n",
       "39       support         support         False      False\n",
       "40          from            from         False       True\n",
       "41           the             the         False       True\n",
       "42      European        European         False      False\n",
       "43         Union           Union         False      False\n",
       "44           and             and         False       True\n",
       "45          Nato            Nato         False      False\n",
       "46            on              on         False       True\n",
       "47        Monday          Monday         False      False\n",
       "48          over            over         False       True\n",
       "49           the             the         False       True"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here we are taking out of the parsed article each token\n",
    "token_text = [token.text for token in parsed_article]\n",
    "\n",
    "# here we are lemmatizing each word possible\n",
    "token_lemmas = [token.lemma_ for token in parsed_article]\n",
    "\n",
    "# stopwords are very common so here we will extract a variable that will tell us whether\n",
    "# a token is a stopword or not\n",
    "token_stop = [token.is_stop for token in parsed_article]\n",
    "\n",
    "# a token is a pinctuation or not\n",
    "token_punc = [token.is_punct for token in parsed_article]\n",
    "\n",
    "# we will now add all three to a dataframe and display it without assigning it to a variable\n",
    "pd.DataFrame(zip(token_text, token_lemmas, token_punc, token_stop), columns=['Original Text', 'Lemmatized Text', 'Punctuations', 'stopwords']).head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acded81",
   "metadata": {},
   "source": [
    "## 4. Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9451715d",
   "metadata": {},
   "source": [
    "Let's start by checking if our dataset contains any missin values, and then evaluate the amount of memory we are currently using from our machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b198ddb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df3460d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d00ab65",
   "metadata": {},
   "source": [
    "Depending on the random sample you choose at the beginning, you may or may not have a lot. If so, getting rid of the columns you don't need will help release some of the memory in your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfcee06",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['url', 'image_url', 'domain'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e183d4f1",
   "metadata": {},
   "source": [
    "Perfect! Let's now extract the `text` column and normalize it. This means we will use `spacy` to,\n",
    "- take out anything that is not a word or a number,\n",
    "- convert to lower case,\n",
    "- strip the spaces around the words,\n",
    "- tokenize the articles,\n",
    "- remove stopwords (we will use spaCy's list of stopwords for this),\n",
    "- and then join the cleaned tokens back together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0547d647",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = df['text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148583c0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "len(STOP_WORDS), STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3cc623",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_doc(doc):\n",
    "    \"\"\"\n",
    "    This function normalizes your list of documents by taking only\n",
    "    words, numbers, and spaces in between them. It then filters out\n",
    "    stop words.\n",
    "    \"\"\"\n",
    "    doc = re.sub(r'[^a-zA-Z0-9\\s]', '', doc, re.I|re.A)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    tokens = nlp(doc)\n",
    "    filtered_tokens = [token.lemma_ for token in tokens if not token.is_stop]\n",
    "    doc = ' '.join(filtered_tokens).replace(\" \\n \", \"\")\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f144da",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "random_article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d60c85",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "normalize_doc(random_article)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a3f444",
   "metadata": {},
   "source": [
    "Since we have quite a few articles, this operation can take quite some time unless we do the cleaning process concurrently or in parallel. We will do this using the `ProcessPoolExecutor()` from the `concurrent.futures` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844c3eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=12) as e:\n",
    "    processed_articles = list(e.map(normalize_doc, articles))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501d7246",
   "metadata": {},
   "source": [
    "We will add the cleaned versions of the documents back into the dataframe and loop over these while taking the lenght (in characters terms) of each article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd33ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df['clean_text'] = processed_articles\n",
    "df['len_clean_text'] = df['clean_text'].apply(len)\n",
    "df['len_dirty_text'] = df['text'].apply(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2430f0",
   "metadata": {},
   "source": [
    "Let's now save our cleaned dataset in case we need to restart our notebook and begin the analysis again. We will also release a bit of memory by getting rid of all the data and variables we have loaded up since the beginning of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a90d73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476466c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['title', 'date', 'clean_text', 'len_clean_text', 'len_dirty_text']].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75a0328",
   "metadata": {},
   "source": [
    "It wouldn't make any sense to feed to our algorithms articles with a tiny amount of characters, so let's examine the distribution of characters among both, the raw and the clean version of our articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113abbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['len_clean_text', 'len_dirty_text']].describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7b5c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['len_clean_text', 'len_dirty_text']].skew()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa97a0c",
   "metadata": {},
   "source": [
    "![img](https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fcdn.analyticsvidhya.com%2Fwp-content%2Fuploads%2F2020%2F06%2Fsk1.png&f=1&nofb=1)\n",
    "\n",
    "Now that we know we have a skewed distribution of characters, let's fix that by setting up a rule. We'll evaluate an article using the tweets' maximum character count of 280, at the time of writing, and filter out all articles with less than that. Let's check how many we have first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f02e8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "greater_than_a_tweet = df['len_clean_text'] > 280\n",
    "greater_than_a_tweet.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18bb1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[greater_than_a_tweet].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550f8f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb689198",
   "metadata": {},
   "source": [
    "# 5. Recommendation System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5604cfd5",
   "metadata": {},
   "source": [
    "Recommendation systems can come in many different forms and sizes. We can create a system that takes into account the behaviour of other users, or a system that only looks at similar articles or items to make a recommendation. Both are powerful systems and could cover an entire section of a book in their own right, which is why we will focus on the latter category, the one that makes recommendations based on similar articles.\n",
    "\n",
    "To create our recommendation system we first need to convert our articles into a numerical representation. We do this with a so-called bag of words (bow). BOWs are matrices with the documents in the rows, the terms contained in all documents along the columns. The frequency with which each term appears in each document along the values can be found in the doc-token combination. To create this kind of representation we can use `sklearn`'s `CountVectorizer` or `TfidfVectorizer` classes. The latter being the normalized version of the former, i.e. the frequency of a word divided by the amount of documents in which it appears.\n",
    "\n",
    "To use this classes we first instantiate them, fit the data to them so that they can learn the vocabulary of our corpus, and then we tranform the corpus into a sparse matrix. These sparse matrices hold the location of all non-zero values to make it easier to store the data and compute on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8db81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# we first instantiate our class\n",
    "tf = TfidfVectorizer(min_df=0.035, max_df=0.80)\n",
    "\n",
    "# we can fit and transform the data in the same step\n",
    "tfidf_matrix = tf.fit_transform(df['clean_text'].values)\n",
    "\n",
    "# evaluate the shape of our matrix\n",
    "tfidf_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368bfa3b",
   "metadata": {},
   "source": [
    "We can access our vocabulary with `.get_feature_names()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaccc68b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf.get_feature_names()[500:550]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e8833b",
   "metadata": {},
   "source": [
    "The next step is to get the distance between documents and words to see how close and how far, based on words only, are two documents from one another. The `cosine_similarity` function we imported earlier can do this for us, and afterwards, we can create a dataframe to evaluate our results.\n",
    "\n",
    "**Note:** this operation can take a few minutes if you are using the entire dataset. Make sure to grab some ☕️ 😎"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d148cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "doc_sim = cosine_similarity(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e0e4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_sim_df = pd.DataFrame(doc_sim)\n",
    "doc_sim_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f7dcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_sim.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d4feee",
   "metadata": {},
   "source": [
    "The reason we see a X000xX000 matrix is because both halfs alonside the diagonal line are identical, hence, we have the similarity of all docs vs all docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5568bf",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "articles_list = df['title'].values\n",
    "articles_list.shape, articles_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5320581",
   "metadata": {},
   "source": [
    "Let's now\n",
    "1. pick a title at random\n",
    "2. get the index of such title\n",
    "3. select the corresponding row for such title in our new document similarity dataframe\n",
    "4. sort the index of such values\n",
    "5. return the top 5 article titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2bfb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebcb555",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_title = choice(articles_list)\n",
    "a_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2ad868",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_idx = np.where(articles_list == a_title)[0][0]\n",
    "article_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4352fa4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "article_similarities = doc_sim_df.iloc[article_idx].values\n",
    "article_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2f7e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that we don't select the first one as this should always be one\n",
    "similar_articles_idxs = np.argsort(-article_similarities)[1:10]\n",
    "similar_articles_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ee2055",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73d7d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = nlp(df.iloc[1, 2])\n",
    "doc2 = nlp(df.iloc[2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e57e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1.similarity(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8434654b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723d08aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_articles = articles_list[similar_articles_idxs]\n",
    "pprint(similar_articles.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9eb754",
   "metadata": {},
   "source": [
    "Lastly, we will create create a mini-dashboard containing,\n",
    "1. a widget with all of our titles,\n",
    "2. a function with the steps we followed above,\n",
    "3. a panel object to store a title, the widget, and the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a841ebe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = df.title.unique().tolist()\n",
    "title_widget = pn.widgets.Select(value=choice(titles), options=titles, name='Articles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d0c2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pn.depends(title_widget.param.value)\n",
    "def article_recommender(title_widget):\n",
    "    \n",
    "    article_idx = np.where(articles_list == title_widget)[0][0]\n",
    "    article_similarities = doc_sim_df.iloc[article_idx].values\n",
    "    similar_title_idxs = np.argsort(-article_similarities)[1:6]\n",
    "    similar_titles = articles_list[similar_title_idxs]\n",
    "    \n",
    "    return pn.Column(*similar_titles, width=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60aaf35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = pn.pane.Markdown(f\"# Small Recommendation Engine\", style={\"color\": \"#000000\"}, width=600, height=50,\n",
    "                        sizing_mode=\"stretch_width\", margin=(10,10,10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b99728",
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.Column(text, title_widget, article_recommender, align='center', width=600, height=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69a8aff",
   "metadata": {},
   "source": [
    "## 6. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6770327e",
   "metadata": {},
   "source": [
    "Blind Spots\n",
    "\n",
    "With additional time we could have,\n",
    "1. Further tweak the parameters of the vectorizers and models;\n",
    "2. Create visualizations of the document similarity to find more interesting patters;\n",
    "3. Take the title of an article out of the body of the article to create a better, less biased representation of the words within a document;\n",
    "4. Using Pytorch's nn.CosineSimilarity would help a lot with increasing the efficiency of our recommendation system;\n",
    "5. There should have been a lemmatization step in the preprocessing stage.\n",
    "\n",
    "Takeaways,\n",
    "1. Recommendation systems are an example of unsupervised machine learning;\n",
    "2. Recommendation systems can be created with or without users behavioural data;\n",
    "3. Creating bags of words requires careful attention to the parameters;\n",
    "4. Where possible, showcase a model or system in a mini-dashboard or data visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d96443",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
